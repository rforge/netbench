---
title: "Netbenchmark"
author: "Pau Bellot, Catharina Olsen, Patrick Meyer"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Netbenchmark}
  %\VignetteEngine{knitr::knitr}
  \usepackage[utf8]{inputenc}
---

In the last decade, several methods have tackled the challenge of 
reconstructing gene regulatory networks from gene expression data. 
Several papers have compared and evaluated the different network inference 
methods relying on a simulated data.

This is new comparison that assesses different methods in a high-heterogeneity 
data scenario which could reveal the specialization of methods for the 
different network types and data. 

This package characterizes the performance, data requirements and noise 
robustness of gene regulatory networks inference methods.  

This package allows replication this comparison between the different 
networks inference algorithms with only one line of code. 

Toy example for main benchmark:
```{r  main_benchmark}
    library(netbenchmark)
    top20.aupr <- netbenchmark(methods="all",datasources.names = "Toy",
                               local.noise=20,global.noise=10,
                               noiseType=c("normal","lognormal"),
                               datasets.num = 2,experiments = 40,
                               seed=1422976420)
```

To generate results of toy noise study:
```{r noise_study}
    noise.aupr <- noise.bench(methods="all",local.noise=seq(0,100,l=3),
                              datasources.names = "Toy",datasets.num=2,
                              experiments = 40,seed=2629)
```

To generate results of the number of experiments study:
```{r experiments_study}
    experiments.aupr <- experiments.bench(methods="all",
                                          datasources.names = "Toy",
                                          experiments=c(20,30,60),
                                          datasets.num=2,seed=4677)
```

The package provides an easy way to compare new techniques with  
state-of-the-art ones and to make new different benchmarks in the future.

First, define the wrapper functions:
```{r wrapper_examples}
    Spearmancor <- function(data){
        cor(data,method="spearman")
    }

    Pearsoncor <- function(data){
        cor(data,method="pearson")
    }
```

Note that the wrapper function returns a matrix which is the weighted 
adjacency matrix of the network inferred by the algorithm and that their 
columns and rows are named.

Evaluate five times these two simple inference methods with syntren300 
datasource:
```{r evaluate_grn}
    res <- netbenchmark(datasources.names="syntren300",
        methods=c("Spearmancor","Pearsoncor"))
    aupr <- res[[1]][,-(1:2)]
```
Plot the $AUPR_{20}$ results:
```{r fig.width=7, fig.height=6}
    boxplot(aupr, main="Syntren300")
```

Plot the mean Precision-Recall curves:
```{r fig.width=7, fig.height=6}
    PR <- res[[5]][[1]]
    col <- rainbow(3)
    plot(PR$rec[,1],PR$pre[,1],type="l",lwd=3,col=col[1],xlab="Recall",
        ylab="Precision",main="Syntren300",xlim=c(0,1),ylim=c(0,1))
    lines(PR$rec[,2],PR$pre[,2],type="l",lwd=3,col=col[2])
    lines(PR$rec[,3],PR$pre[,3],type="l",lwd=3,col=col[3])
    legend("topright", inset=.05,title="Method",colnames(PR$rec),fill=col)
```